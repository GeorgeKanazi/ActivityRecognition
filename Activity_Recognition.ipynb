{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.multiprocessing as mp\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Nm7FCx2s_xtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZluXgG6_1aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/bgu-i-know-what-you-did-last-measurement-time.zip', 'r') as zip_ref:\n",
        "        # Extract all contents to the specified directory\n",
        "        zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "YgnJhU-r_2Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Set Implemntation**"
      ],
      "metadata": {
        "id": "78rlRaHUS0-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDataSet(Dataset):\n",
        "    def __init__(self, csv_file, train=True):\n",
        "        self.sequences = pd.read_csv(csv_file)\n",
        "        self.data = self.sequences['Sequence_Path']\n",
        "        self.targets = self.sequences['Label']\n",
        "        self.noise_std = 0.01\n",
        "        self.padding = 4000\n",
        "        self.train = train\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_path = self.data[idx]\n",
        "        sequence =  pd.read_csv(sequence_path)\n",
        "        label = self.targets[idx]\n",
        "\n",
        "        if len(sequence.columns) == 3:\n",
        "            seq_tens = torch.tensor(sequence.values)\n",
        "        else:\n",
        "            accelration = sequence[sequence['measurement type'] == 'acceleration [m/s/s]']\n",
        "            accelration = accelration.drop(columns='measurement type')\n",
        "            seq_tens = torch.tensor(accelration.values)\n",
        "        if self.train:\n",
        "          seq_tens = seq_tens + torch.randn_like(seq_tens) * self.noise_std\n",
        "        if seq_tens.shape[0] != 4000:\n",
        "           padding = 4000 - seq_tens.shape[0]\n",
        "          #paddingLeft = torch.zeros((padding // 2, seq_tens.size(1)))\n",
        "           paddingRight = torch.zeros((padding, seq_tens.size(1)))\n",
        "           seq_tens = torch.cat([seq_tens, paddingRight], dim=0)\n",
        "\n",
        "        tensor_float32 = seq_tens.to(dtype=torch.float32)\n",
        "        return tensor_float32, label"
      ],
      "metadata": {
        "id": "qgTg5UJeS0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loaders**"
      ],
      "metadata": {
        "id": "44QpY9lpSntU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SequenceDataSet('/content/train_data.csv')\n",
        "\n",
        "val_size = 0.2\n",
        "\n",
        "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=val_size, random_state=42)\n",
        "\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "batch_size = 32\n",
        "shuffle_train = True\n",
        "shuffle_val = False\n",
        "num_workers = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle_val)"
      ],
      "metadata": {
        "id": "rk6hVl3FSlKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Test Functions**"
      ],
      "metadata": {
        "id": "8EVQLMrTTBQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, dataloader, optimizer, criterion, device='cuda'):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    model.to(device)\n",
        "    for step, (sequence, target) in enumerate(dataloader):\n",
        "        sequence, target = sequence.to(device),target.to(device)\n",
        "        # getting the output of the model\n",
        "        logits = model(sequence)\n",
        "        # getting the cost of the model\n",
        "        loss = criterion(logits, target)\n",
        "        # Backpropagation and updating the weights\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "def test(model, dataloader, criterion, device='cuda'):\n",
        "    avg_acc, avg_loss = 0, 0\n",
        "    model.to(device)\n",
        "    for seq,target in dataloader:\n",
        "        seq,target = seq.to(device),target.to(device)\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, target)\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "\n",
        "    return avg_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "rVHt5qvjTA7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Model**"
      ],
      "metadata": {
        "id": "z0CmHpsTTdtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, inputdim, hidden_dim, output_dim, num_layers, dropout=0):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out,_ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "bHRmA8H4Tc90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple CNN Model**"
      ],
      "metadata": {
        "id": "hWBd2_RMTiJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size=3, stride=1, padding=1):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=kernel_size, stride=stride, padding=padding)  # Adjusted input channels\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(256 * (seq_len // 2), output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, 3, seq_len)  # Adjusted input channels\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        x = self.conv1d(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1d2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1d3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten for fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HWSp3ymEThuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improved CNN**"
      ],
      "metadata": {
        "id": "cCWZBw9yTrNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1DIMP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size=3, stride=1, padding=1, dropout_prob=0.5):\n",
        "        super(CNN1DIMP, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # Batch normalization layer after the first convolution\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.bn2 = nn.BatchNorm1d(128)  # Batch normalization layer after the second convolution\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.bn3 = nn.BatchNorm1d(256)  # Batch normalization layer after the third convolution\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(256 * (seq_len // 2), output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x, 1, 2)  # Adjust input shape\n",
        "        x = self.conv1d(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1d2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1d3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "y6WKa-MLTuzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Supervised**"
      ],
      "metadata": {
        "id": "FZ8M5MYAUHwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto Encoder**"
      ],
      "metadata": {
        "id": "m8HY6ouFUSET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, latent_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_size, input_size),\n",
        "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "uR_a447SUGsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple GRU Training**"
      ],
      "metadata": {
        "id": "8zAQDlYBUmw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 3\n",
        "hidden_dim = 32\n",
        "output_dim = 18\n",
        "num_layers = 5\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = GRUClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_loss, v_loss = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train of each epoch:\n",
        "    epoch_loss = train_step(model, train_dataloader, optimizer, criterion, device='cuda')\n",
        "    model.eval()\n",
        "    #save the results of each epoch\n",
        "    loss = test(model, train_dataloader, criterion, device='cuda')\n",
        "    val_loss = test(model, val_dataloader, criterion, device='cuda')\n",
        "    train_loss.append(loss)\n",
        "    v_loss.append(val_loss)\n",
        "    print(f'Epoch [{epoch+1}], Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "g26uKfiTUrlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Curve for simple GRU**"
      ],
      "metadata": {
        "id": "PYf3DlYCVoOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, 11)\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, v_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uM0BP0wzVnp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple CNN Training**"
      ],
      "metadata": {
        "id": "M7KCWem-V2Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "model = CNN1D(3,18)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_loss, v_loss = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train of each epoch:\n",
        "    epoch_loss = train_step(model, train_dataloader, optimizer, criterion, device='cuda')\n",
        "    model.eval()\n",
        "    #save the results of each epoch\n",
        "    loss = test(model, train_dataloader, criterion, device='cuda')\n",
        "    val_loss = test(model, val_dataloader, criterion, device='cuda')\n",
        "    train_loss.append(loss)\n",
        "    v_loss.append(val_loss)\n",
        "    print(f'Epoch [{epoch+1}], Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "XMq1DPA2V1n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Curve for simple CNN**"
      ],
      "metadata": {
        "id": "eKV09rfSWDzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, 11)\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, v_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8vIurL5rWDMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improved GRU Training**"
      ],
      "metadata": {
        "id": "-Jyh9USAWMTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 3\n",
        "hidden_dim = 32\n",
        "output_dim = 18\n",
        "num_layers = 5\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = GRUClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
        "\n",
        "train_loss, v_loss = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train of each epoch:\n",
        "    epoch_loss = train_step(model, train_dataloader, optimizer, criterion, device='cuda')\n",
        "    model.eval()\n",
        "    #save the results of each epoch\n",
        "    loss = test(model, train_dataloader, criterion, device='cuda')\n",
        "    val_loss = test(model, val_dataloader, criterion, device='cuda')\n",
        "    train_loss.append(loss)\n",
        "    v_loss.append(val_loss)\n",
        "    scheduler.step()\n",
        "    print(f'Epoch [{epoch+1}], Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "o7WEEhOvWLzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Curve for Improved GRU**"
      ],
      "metadata": {
        "id": "c9GUGVfcWdgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, 21)\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, v_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RCNBIta7WdG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improved CNN Training**"
      ],
      "metadata": {
        "id": "BzRXKo0WWoFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "model = CNN1DIMP(3,18)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_loss, v_loss = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #train of each epoch:\n",
        "    epoch_loss = train_step(model, train_dataloader, optimizer, criterion, device='cuda')\n",
        "    model.eval()\n",
        "    #save the results of each epoch\n",
        "    loss = test(model, train_dataloader, criterion, device='cuda')\n",
        "    val_loss = test(model, val_dataloader, criterion, device='cuda')\n",
        "    train_loss.append(loss)\n",
        "    v_loss.append(val_loss)\n",
        "    print(f'Epoch [{epoch+1}], Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "Bv448uc2Wnp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Curve For Improved CNN**"
      ],
      "metadata": {
        "id": "gU780AbJW79y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, 11)\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Training Loss')\n",
        "plt.plot(epochs, v_loss, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LMRtbIXcXBI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self Supervised Training**"
      ],
      "metadata": {
        "id": "jxy9Uk6cXEI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "input_size = 3  # Dimensionality of each time step in the sequence\n",
        "latent_size = 2  # Dimensionality of the latent space\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Create data loader\n",
        "dataset = SequenceDataSet('/content/train_data.csv')\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Step 4: Create Data Loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = Autoencoder(input_size, latent_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to('cuda')\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for input, _ in train_dataloader:\n",
        "        input = input.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch = model(input)\n",
        "        loss = criterion(recon_batch, input)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_dataloader.dataset)}\")\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for input, _ in val_dataloader:\n",
        "      input = input.to('cuda')\n",
        "      with torch.no_grad():\n",
        "        recon_batch = model(input)\n",
        "        loss = criterion(recon_batch, input)\n",
        "        val_loss += loss\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Vall Loss: {train_loss / len(val_dataloader.dataset)}\")\n",
        "\n",
        "\n",
        "# Save the encoder part of the trained autoencoder\n",
        "torch.save(model.encoder.state_dict(), 'encoder.pth')"
      ],
      "metadata": {
        "id": "lLbLp_mYXH-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tunning the Clasifier**"
      ],
      "metadata": {
        "id": "oSKZ2oXaXaa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for classifier training\n",
        "num_classes = 18  # Number of classes for classification\n",
        "classifier_learning_rate = 0.001\n",
        "classifier_num_epochs = 10\n",
        "\n",
        "# Load the pre-trained encoder\n",
        "pretrained_encoder = nn.Sequential(\n",
        "    nn.Linear(input_size, latent_size),\n",
        "    nn.ReLU()\n",
        ")\n",
        "pretrained_encoder.load_state_dict(torch.load('encoder.pth'))\n",
        "\n",
        "\n",
        "# Initialize classifier model, loss function, and optimizer\n",
        "classifier_model = GRUModel(latent_size, hidden_size=32, num_layers=5, num_classes=18)\n",
        "classifier_criterion = nn.CrossEntropyLoss()\n",
        "classifier_optimizer = optim.Adam(classifier_model.parameters(), lr=classifier_learning_rate)\n",
        "val_loss_lst, train_loss_lst = [], []\n",
        "# Training loop for classifier\n",
        "classifier_model.to('cuda')\n",
        "pretrained_encoder.to('cuda')\n",
        "for epoch in range(classifier_num_epochs):\n",
        "    total_loss = 0\n",
        "    classifier_model.train()\n",
        "    for inputs, labels in train_dataloader:\n",
        "        #inputs = inputs.view(-1, input_size)  # Flatten input sequences\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        encoded = pretrained_encoder(inputs)  # Use pre-trained encoder for feature extraction\n",
        "        classifier_optimizer.zero_grad()\n",
        "        outputs = classifier_model(encoded)\n",
        "        loss = classifier_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        classifier_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    train_loss_lst.append(total_loss/len(train_dataloader.dataset))\n",
        "    print(f\"Classifier Epoch {epoch + 1}, Train Loss: {total_loss / len(train_dataloader.dataset)}\")\n",
        "\n",
        "    classifier_model.eval()\n",
        "    val_loss = 0\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "      with torch.no_grad():\n",
        "        encoded = pretrained_encoder(inputs)  # Use pre-trained encoder for feature extraction\n",
        "        outputs = classifier_model(encoded)\n",
        "        loss = classifier_criterion(outputs, labels)\n",
        "        val_loss += loss\n",
        "    val_loss_lst.append(val_loss/len(val_dataloader.dataset))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Vall Loss: {val_loss / len(val_dataloader.dataset)}\")"
      ],
      "metadata": {
        "id": "_OZV-nFLXVgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for detecting good and bad clasification of the model**"
      ],
      "metadata": {
        "id": "81Dr4dDPX3T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 3\n",
        "hidden_dim = 32\n",
        "output_dim = 18  # Number of classes\n",
        "num_layers = 5\n",
        "\n",
        "model = GRUClassifier(input_dim, hidden_dim, output_dim, num_layers,0)\n",
        "\n",
        "# Load the state dictionary from the .pth file\n",
        "state_dict = torch.load('BasicGRU.pth')\n",
        "\n",
        "# Load the state dictionary into your model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "bad = []\n",
        "good = []\n",
        "\n",
        "#Iterate over the test loader\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        predictions = probabilities.argmax(dim=1)\n",
        "        for i in range(32):\n",
        "          if probabilities.shape[0] == 32:\n",
        "            if predictions[i] != targets[i]:\n",
        "              if probabilities[i][int(targets[i])] < 0.01:\n",
        "                bad.append([targets[i], predictions[i], probabilities[i][int(targets[i])]])\n",
        "\n",
        "            elif predictions[i] == targets[i]:\n",
        "              if probabilities[i][int(targets[i])] > 0.93:\n",
        "                good.append([targets[i], probabilities[i][int(targets[i])]])"
      ],
      "metadata": {
        "id": "8MuTcgQKX9fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Good Clasification**"
      ],
      "metadata": {
        "id": "qC4nSjtyYBg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = {i: 0 for i in range(0, 18)}  # Initialize with zeros from 1 to 18\n",
        "\n",
        "# Iterate through the data list and count occurrences of each value\n",
        "for row in good:\n",
        "    value = row[0].item()\n",
        "    value_counts[value] += 1\n",
        "\n",
        "# Extract keys and values\n",
        "values = list(value_counts.keys())\n",
        "counts = list(value_counts.values())\n",
        "\n",
        "# Plot bar plot for each value\n",
        "plt.bar(values, counts)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bar Plot for Each Value for Good Classification')\n",
        "plt.xticks(range(0, 18))  # Set x-axis ticks from 1 to 18\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wWgw3zVdX-O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Bad Classification**"
      ],
      "metadata": {
        "id": "1AgFSQlQYFMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = {i: 0 for i in range(0, 18)}  # Initialize with zeros from 1 to 18\n",
        "\n",
        "# Iterate through the data list and count occurrences of each value\n",
        "for row in bad:\n",
        "    value = row[0].item()\n",
        "    value_counts[value] += 1\n",
        "\n",
        "# Extract keys and values\n",
        "values = list(value_counts.keys())\n",
        "counts = list(value_counts.values())\n",
        "\n",
        "# Plot bar plot for each value\n",
        "plt.bar(values, counts)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bar Plot for Each Label for Bad Classification')\n",
        "plt.xticks(range(0, 18))  # Set x-axis ticks from 1 to 18\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HNHO_AvLYJez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical ML**"
      ],
      "metadata": {
        "id": "Vtkw1xLj-FJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Set**"
      ],
      "metadata": {
        "id": "p-TmDcOE-KW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOb1MFW-9_z9"
      },
      "outputs": [],
      "source": [
        "class SequenceDataSetML(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.sequences = pd.read_csv(csv_file)\n",
        "        self.data = self.sequences['Sequence_Path']\n",
        "        self.targets = self.sequences['Label']\n",
        "        self.noise_std = 0.01\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence_path = self.data[idx]\n",
        "        sequence =  pd.read_csv(sequence_path)\n",
        "        label = self.targets[idx]\n",
        "\n",
        "        if len(sequence.columns) == 3:\n",
        "            seq_tens = torch.tensor(sequence.values)\n",
        "        else:\n",
        "            accelration = sequence[sequence['measurement type'] == 'acceleration [m/s/s]']\n",
        "            accelration = accelration.drop(columns='measurement type')\n",
        "            seq_tens = torch.tensor(accelration.values)\n",
        "\n",
        "        seq_tens = seq_tens + torch.randn_like(seq_tens) * self.noise_std\n",
        "        seq_len = len(seq_tens)\n",
        "        subtensors = torch.chunk(seq_tens, 10, dim=0)\n",
        "        subtensor_stats = [seq_len]\n",
        "        for subtensor in subtensors:\n",
        "            mean = subtensor.mean(dim=0).tolist()\n",
        "            std = subtensor.std(dim=0).tolist()\n",
        "            for i in range(3):\n",
        "              subtensor_stats.append(mean[i])\n",
        "              subtensor_stats.append(std[i])\n",
        "\n",
        "\n",
        "\n",
        "        return subtensor_stats, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**"
      ],
      "metadata": {
        "id": "lo9wYvQX-ZMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SequenceDataSetML('/content/train_data (1).csv')\n",
        "\n",
        "# Define the proportion of samples for the validation set\n",
        "val_size = 0.2\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=val_size, random_state=42)\n",
        "\n",
        "# Create Subset instances for the training and validation sets\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "# Define DataLoader parameters\n",
        "batch_size = 32\n",
        "shuffle_train = True\n",
        "shuffle_val = False\n",
        "num_workers = 2\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle_val)"
      ],
      "metadata": {
        "id": "unM3s2dq-Zk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction**"
      ],
      "metadata": {
        "id": "rCoWCK13-g5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = xgb.XGBClassifier()\n",
        "features_list_train, labels_list_train = [], []\n",
        "features_list_val, labels_list_val = [], []\n",
        "\n",
        "for step, (sequence_features, target) in enumerate(train_dataloader):\n",
        "    features_list_train.append(np.array(sequence_features).T) # Convert features to NumPy array\n",
        "    labels_list_train.append(np.array(target).T) # Convert labels to NumPy array\n",
        "\n",
        "features_array_train = np.concatenate(features_list_train[:-1], axis=0)\n",
        "labels_array_train = np.concatenate(labels_list_train[:-1], axis=0)\n",
        "\n",
        "for step, (sequence_features, target) in enumerate(val_dataloader):\n",
        "    features_list_val.append(np.array(sequence_features).T)  # Convert features to NumPy array\n",
        "    labels_list_val.append(np.array(target).T)  # Convert labels to NumPy array\n",
        "\n",
        "\n",
        "features_array_val = np.concatenate(features_list_val[:-1], axis=0)\n",
        "labels_array_val = np.concatenate(labels_list_val[:-1], axis=0)"
      ],
      "metadata": {
        "id": "ZwtThRlG-gNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Model**"
      ],
      "metadata": {
        "id": "Uq5aiRBB-q6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', solver='liblinear', multi_class='auto'))\n",
        "\n",
        "true_train, true_val = 0, 0\n",
        "pipeline.fit(features_array_train, labels_array_train)\n",
        "y_hat_train = pipeline.predict(features_array_train)\n",
        "true_train += (y_hat_train == labels_array_train).sum()\n",
        "y_hat_val = pipeline.predict(features_array_val)\n",
        "true_val += (y_hat_val == labels_array_val).sum()\n",
        "\n",
        "print(f'Training Acc: {true_train/len(train_dataloader.dataset):.4f}, Validation Acc: {true_val/len(val_dataloader.dataset):.4f}')\n"
      ],
      "metadata": {
        "id": "JskQKmiq-rNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Model**"
      ],
      "metadata": {
        "id": "n57Xtkpl-zzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "true_train, true_val = 0, 0\n",
        "model.fit(features_array_train, labels_array_train)\n",
        "y_hat_train = model.predict(features_array_train)\n",
        "true_train += (y_hat_train == labels_array_train).sum()\n",
        "y_hat_val = model.predict(features_array_val)\n",
        "true_val += (y_hat_val == labels_array_val).sum()\n",
        "\n",
        "print(f'Training Acc: {true_train/len(train_dataloader.dataset):.4f}, Validation Acc: {true_val/len(val_dataloader.dataset):.4f}')\n"
      ],
      "metadata": {
        "id": "CKzskA6E-5lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGB Model**"
      ],
      "metadata": {
        "id": "O87X6II0-913"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = xgb.XGBClassifier()\n",
        "\n",
        "true_train, true_val = 0, 0\n",
        "model.fit(features_array_train, labels_array_train)\n",
        "y_hat_train = model.predict(features_array_train)\n",
        "true_train += (y_hat_train == labels_array_train).sum()\n",
        "y_hat_val = model.predict(features_array_val)\n",
        "true_val += (y_hat_val == labels_array_val).sum()\n",
        "\n",
        "print(f'Training Acc: {true_train/len(train_dataloader.dataset):.4f}, Validation Acc: {true_val/len(val_dataloader.dataset):.4f}')\n"
      ],
      "metadata": {
        "id": "QMI2ooKs_CIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}